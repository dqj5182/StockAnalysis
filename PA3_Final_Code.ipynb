{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Import libraries"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#make sure we install scikit-learn before we import \n!pip install -U scikit-learn",
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already up-to-date: scikit-learn in /Users/daehoon/opt/anaconda3/lib/python3.7/site-packages (0.22.2.post1)\nRequirement already satisfied, skipping upgrade: numpy>=1.11.0 in /Users/daehoon/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.17.2)\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/daehoon/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (0.13.2)\nRequirement already satisfied, skipping upgrade: scipy>=0.17.0 in /Users/daehoon/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.3.1)\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport sklearn\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets.samples_generator import make_blobs\n\nfrom sklearn.metrics.cluster import contingency_matrix\nfrom scipy.special import comb\nimport matplotlib.pyplot as plt\n\nimport pylab as pl\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.impute import KNNImputer",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Clustering matrix function"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def clustering_metrics(y_true, y_pred):\n    \n    # obtain contingency matrix: P * C\n    # P is the number of ground truth clusters\n    # C is the number of clusters produced by the alogrithm\n    cm = contingency_matrix(y_true, y_pred)\n    \n    # Please refer to the page: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.comb.html\n    # We can use the comb function to calculate “N choose k”\n    #\n    # Example:\n    # from scipy.special import comb\n    # N = np.array([4, 5])\n    # result = comb(N, 2)\n    # print(result)\n    #\n    # Outputs:\n    # [ 6. 10.]\n    \n    # You may also use the following functions: \n    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html\n    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html\n    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.amax.html\n    \n    # For rand index and F1\n    # step 1: tp + fp\n    tp_plus_fp = np.sum(comb(np.sum(cm,axis=0),2))\n    \n    # step 2: tp + fn\n    tp_plus_fn = np.sum(comb(np.sum(cm,axis=1),2))\n    \n    # step 3: tp\n    tp = np.sum(comb(cm,2))\n    \n    # step 4: fp, fn, tn\n    fp = tp_plus_fp - tp\n    fn = tp_plus_fn - tp\n    tn = comb(np.sum(cm),2) - tp - fp - fn\n    \n    # rand index\n    rand_index = (tp + tn) / (tp + fp + fn + tn)\n    # F1\n    F1 = 2 * tp / (2 * tp + fp + fn)\n    \n    #-------------------\n    # For purity  \n    numerator = np.sum(cm.max(axis=0))\n    denominator = np.sum(cm)\n    purity = numerator / denominator\n    \n    return rand_index, F1, purity",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Import training and testing data set  "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Import data\ndf_train = pd.read_csv('train_data.csv', sep=',')\ndf_test = pd.read_csv('test_data.csv', sep=',')",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Data pre-processing"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "1. Balance the number of each classes"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Make sure class = 0 and class = 1 balances\n\n# Put data rows with class = 0 in df_train0\ndf_train0 = df_train[df_train['Class'] == 0]\n# Put data rows with class = 1 in df_train1\ndf_train1 = df_train[df_train['Class'] == 1]",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Sample number of rows (same with numer of rows of df_train1) from df_train0 \ndf_train0 = df_train0.sample(n = df_train1.shape[0], replace = False)",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Concatenate df_train0 and df_train1\ndf_train = pd.concat([df_train0, df_train1], axis = 0)",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "2. Factorize \"Sector\" column for both training and testing data set"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Factorize sectors and put it into newly created Sector_fac column\ndf_train['Sector_fac'] = pd.factorize(df_train['Sector'])[0]\nsector_index = pd.factorize(df_train['Sector'])[1]\nsector_dict = {k: v for v, k in enumerate(sector_index)}",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Replace Sector column values with sector_dict (dictionary) reated above\ndf_test.replace(sector_dict, inplace = True)",
      "execution_count": 35,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "train_data = df_train.values\ntest_data = df_test.values",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "train_labels = list(train_data[:, -2])",
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Minor adjustment for each data set"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Delete company name from train_features\ntrain_features = np.delete(train_data, 0, 1)\n# Delete class column from train_features\ntrain_features = np.delete(train_features, -2, 1)\n# Delete sector column from train_features (adjusted)\ntrain_features = np.delete(train_features, -2, 1)\n\n# Delete company name from test_features\ntest_features = np.delete(test_data, 0, 1)",
      "execution_count": 38,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Imputation"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# preprocessing\n# You can try any preprocessing approaches\nimp = KNNImputer(n_neighbors=5, weights='distance') # impute the data with knn value\nimp.fit(train_features)\nKNNImputer()\nimputed_train_features = imp.transform(train_features)\nimputed_test_features = imp.transform(test_features)",
      "execution_count": 39,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Feature engineering"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sel = VarianceThreshold(threshold = (.6 * (1 - .6)))\nreduced_features = sel.fit_transform(imputed_train_features)",
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Only 200 columns left\nreduced_features.shape",
      "execution_count": 47,
      "outputs": [
        {
          "data": {
            "text/plain": "(1922, 198)"
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Originally, there were 222 columns\nimputed_train_features.shape",
      "execution_count": 48,
      "outputs": [
        {
          "data": {
            "text/plain": "(1922, 222)"
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our main classifier (with all columns included)"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "imputed_test_features.shape",
      "execution_count": 49,
      "outputs": [
        {
          "data": {
            "text/plain": "(1488, 222)"
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "imputed_train_features.shape",
      "execution_count": 50,
      "outputs": [
        {
          "data": {
            "text/plain": "(1922, 222)"
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(imputed_train_features, train_labels, test_size = 0.2, random_state = 100)",
      "execution_count": 51,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(reduced_features, train_labels, test_size = 0.2, random_state = 100)",
      "execution_count": 52,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# classifier\n# training\nclf = KNeighborsClassifier(n_neighbors=5) # KNN classifier\nclf.fit(X_train, y_train)\n# prediction\npredictions = clf.predict(X_test)",
      "execution_count": 53,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# evaluation metrics\nrand_index, F1, purity = clustering_metrics(y_test, predictions)\nprint('Kmeans: K =', 5)\nprint('Rand index =', rand_index)\nprint('F1 =', F1)\nprint('Purity =', purity)",
      "execution_count": 55,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Kmeans: K = 5\nRand index = 0.5096861471861471\nF1 = 0.5092612651646448\nPurity = 0.574025974025974\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Experimental classifier (with reduced columns included)"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# classifier\n# training\nclf_temp = KNeighborsClassifier(n_neighbors=5) # KNN classifier\nclf_temp.fit(X_train_temp, y_train_temp)\n# prediction\npredictions_temp = clf_temp.predict(X_test_temp)",
      "execution_count": 56,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# evaluation metrics\nrand_index_temp, F1_temp, purity_temp = clustering_metrics(y_test_temp, predictions_temp)\nprint('Kmeans: K =', 5)\nprint('Rand index =', rand_index_temp)\nprint('F1 =', F1_temp)\nprint('Purity =', purity_temp)",
      "execution_count": 57,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Kmeans: K = 5\nRand index = 0.5096861471861471\nF1 = 0.5092612651646448\nPurity = 0.574025974025974\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Final model"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid = GridSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1)\n\n## Fitting the model. \ngrid.fit(imputed_train_features,train_labels)",
      "execution_count": 40,
      "outputs": [
        {
          "data": {
            "text/plain": "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=15, test_size=0.3,\n            train_size=None),\n             error_score=nan,\n             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                            metric='minkowski',\n                                            metric_params=None, n_jobs=None,\n                                            n_neighbors=5, p=2,\n                                            weights='uniform'),\n             iid='deprecated', n_jobs=-1,\n             param_grid={'n_neighbors': range(1, 31),\n                         'weights': ['uniform', 'distance']},\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n             scoring=None, verbose=False)"
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#show best parameters\ngrid.best_params_",
      "execution_count": 41,
      "outputs": [
        {
          "data": {
            "text/plain": "{'n_neighbors': 26, 'weights': 'distance'}"
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# classifier\n# You can try any classifiers\n# training\nclf = KNeighborsClassifier(n_neighbors=26,weights='distance') # KNN classifier\nclf.fit(imputed_train_features, train_labels)\n\n#show accuracy\nacc_knn = round(clf.score(imputed_train_features, train_labels) * 100, 2)\nprint(acc_knn)\n\n# prediction\npredictions = clf.predict(imputed_test_features)",
      "execution_count": 43,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "98.13\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# write to submission file\ndf_sub = pd.read_csv('sampleSubmission.csv', sep=',')\ndf_sub['Class'] = predictions\ndf_sub.to_csv ('final_submission2.csv', index = False, header=True)\nprint('done!')",
      "execution_count": 467,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "done!\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
      "language": "python",
      "name": "python37464bitanaconda3virtualenv16a0b46fcae948c4b701c51cd118f977"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}